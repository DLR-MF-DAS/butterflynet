"""
This code is generated by Ridvan Salih KUZU @DLR
LAST EDITED:  01.06.2021
ABOUT SCRIPT:
It defines training, validation and evaluation data loader classes and functions
"""

import numpy as np
import pandas as pd
from tensorflow.keras.utils import Sequence
from tensorflow.keras.preprocessing.image import img_to_array,load_img
from numpy.random import randint
import glob
from sklearn.model_selection import train_test_split
from pathlib import Path
import random
import tensorflow as tf

class TrainValidTestGenerator():
    """
    THIS CLASS ORCHESTRATES THE TRAINING, VALIDATION, AND TEST DATA GENERATORS
    """
    def __init__(self, data_dir, label_dir, pair_csv,
                 workers=4,
                 multi_gpu=True,
                 image_shape = (128, 128, 1),
                 mask_shape = (128, 128, 1),
                 transformation=(None,None),
                 augment_time=10,
                 batch_size=4,
                 is_shuffle=True,
                 self_supervised=False,
                 self_reconstruct=False,
                 valid_test_split=(0.1,0.1)):

        self.multi_gpu=multi_gpu
        self.workers=workers
        self.batch_size=batch_size

        train_frame,valid_frame,test_frame = self.splitter(data_dir, label_dir, pair_csv, valid_test_split, is_shuffle, self_supervised)

        self._generators = {
            'train': DataGenerator(train_frame, transformation=transformation[0], image_shape=image_shape, mask_shape=mask_shape, augment_time=augment_time, batch_size=batch_size, is_shuffle=is_shuffle, self_supervised=self_supervised, self_reconstruct=self_reconstruct),
            'valid': DataGenerator(valid_frame, transformation=transformation[1], image_shape=image_shape, mask_shape=mask_shape, augment_time=augment_time, batch_size=batch_size, is_shuffle=is_shuffle, self_supervised=self_supervised, self_reconstruct=self_reconstruct),
            'test':  DataGenerator(test_frame,  transformation=transformation[2], image_shape=image_shape, mask_shape=mask_shape, augment_time=augment_time, batch_size=batch_size, is_shuffle=is_shuffle, self_supervised=self_supervised, self_reconstruct=self_reconstruct)}


    def get_generator(self, type='train'):
        '''
           THIS FUNCTION SELECT THE GENERATOR TO BE RETURNED BY CONSIDERING IF TRAINING IS DISTRIBUTED OVER MULTIPLE GPU OR NOT.
           :param type: 'train', 'valid' or 'test' type
           :return: returns one of the generator type
          '''
        gen=self._generators[type]
        if self.multi_gpu:
            gen=self.multi_generator(gen)
        return gen

    def get_steps(self,type='train'):
        '''
        THIS FUNCTION RETURNS THE SIZE OF THE SAMPLES IN A SELECTED GENERATOR.
        :param type: 'train', 'valid' or 'test' type
        :return: returns the samples size the generator type
        '''
        return self._generators[type].__len__()


    def multi_generator(self,data_gen):
        '''
        THIS FUNCTION CONVERTS DATA GENERATOR FOR MULTIPLE GPU COMPATIBILITY.
        :param data_gen: 'train', 'valid' or 'test' generator
        :return: returns the generator with multi-gpu distribution policy
        '''
        dataset = tf.data.Dataset.from_generator(data_gen.generator,
                                                 output_types=(tf.float64, tf.float64),
                                                 output_shapes=(tf.TensorShape([ None, None, None, None]),
                                                                tf.TensorShape([ None, None, None, None])))
        options = tf.data.Options()
        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
        return dataset.with_options(options)


    def splitter(self, data_dir, label_dir, pair_csv, valid_test_split, is_shuffle, self_supervised):
        '''
        THIS FUNCTION SPLITS THE DATA INTO TRAINING, VALIDATION AND TEST PARTITIONS.
        :param data_dir: folder directory for input images
        :param label_dir: folder directory for input labels
        :param pair_csv: if standard supervised learning, it is the pair list of data and label samples in two columns.
               It is not required in self-supervised learning
        :param valid_test_split: it is a tuple to determine validation and test partitions.
               if the tuples are less than 1, it means percentage, e.g. (0.1, 0.1) means 80-10-10 percent for training validation and test.
               if the tuples are greater than 1, it means number, e.g. (32, 32) means 32 of samples are reserved for validation and test, and the rest for training.
        :return: returns the training, validation and test partitions as the lists of filenames
        '''
        random.seed(42)
        if pair_csv is not None:
            df = pd.read_csv(pair_csv)
            images = [data_dir + file.strip() for file in  df[df.columns[1]].values]
            if self_supervised:
                masks = [label_dir + file.strip() for file in df[df.columns[1]].values]
            else:
                masks = [label_dir + file.strip() for file in df[df.columns[2]].values]
        else:
            images = glob.glob(data_dir + '/*.bmp')
            masks = glob.glob(label_dir + '/*.bmp')

        if len(images) == len(masks):
            x_train, x_test, y_train, y_test = train_test_split(images,masks, test_size=valid_test_split[1], shuffle=is_shuffle)
            x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,test_size=valid_test_split[0], shuffle=is_shuffle)

            train_frame = (x_train,y_train)
            valid_frame = (x_valid, y_valid)
            test_frame = (x_test, y_test)

            return train_frame,valid_frame,test_frame
        else:
            raise Exception('number of images {} and number of masks {} are not equal! '.format(len(images), len(masks)))


#######################################################################################################################

class DataGenerator(Sequence):
    """
    THIS CLASS BUILDS A DATA GENERATOR FOR UNET-BASED ARCHITECTURES.
    """
    def __init__(self, data_frame,
                 image_shape = (128, 128, 3),
                 mask_shape = (128, 128, 1),
                 transformation=None,
                 augment_time=1,
                 batch_size=4,
                 is_shuffle=True,
                 self_supervised=False,
                 self_reconstruct=False):

        self.batch_size = batch_size
        self.transform = transformation
        self.augment_time=augment_time
        self.is_shuffle = is_shuffle
        self.image_shape = image_shape
        self.mask_shape=mask_shape
        self.self_supervised=self_supervised
        self.self_reconstruct = self_reconstruct

        self.image_color_mode= 'rgb' if image_shape[2] == 3 else 'grayscale'
        self.mask_color_mode = 'rgb' if mask_shape[2] == 3 else 'grayscale'

        self.data_list =self.augment_data(data_frame)
        self.indexes = np.arange(len(self.data_list))

        self.on_epoch_end()

    def on_epoch_end(self):
        '''THIS FUNCTION UPDATES THE INDEXES AFTER EACH EPOCH.'''
        if self.is_shuffle == True:
            np.random.shuffle(self.indexes)

    def augment_data(self,data_frame):
        '''
        THIS FUNCTION ENLARGES THE DATA SIZE FOR THE AUGMENTATION.
        :param data_frame: the tuple list of input files and label files
        :return: returns the enlarged list of input files and label files
        '''

        im_list =[]
        mask_list=[]
        for i in range(self.augment_time):
            im_list.extend(data_frame[0])
            mask_list.extend(data_frame[1])

        return pd.DataFrame({'image':im_list,'mask':mask_list})


    def __len__(self):
        '''THIS FUNCTION DENOTES THE NUMBER OF BATCHES PER EPOCH.'''
        return int(np.floor(len(self.indexes)/self.batch_size))

    def __getitem__(self, index):
        '''THIS FUNCTION GENERATES ONE BATCH OF DATA FOR SINGLE GPU LEARNING'''

        X = np.empty((self.batch_size, *self.image_shape))
        Y = np.empty((self.batch_size, *self.mask_shape))

        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        if not self.self_supervised: # IF STANDARD SUPERVISED LEARNING, RETURNS ONLY IMAGE AND MASK PAIRS
            for i, ID in enumerate(indexes):
                X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
            return X,Y
        else: # IF SELF SUPERVISED LEARNING
            if self.self_reconstruct: # IF SELF-RECONSTRUCTION LEARNING, RETURNS ONLY IMAGE - IMAGE PAIRS
                for i, ID in enumerate(indexes):
                    X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
                return X, Y
            else: # IF SIMCLR LEARNING, RETURNS IMAGE - IMAGE PAIRS TOGETHER WITH BINARY LABELS
                z = np.empty((self.batch_size, 1))
                for i, ID in enumerate(indexes[:len(indexes)//2]):
                    X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
                    z[i,]=0 # HALF OF THE BATCH IS PREPARED WITH THE DIFFERENT AUGMENTATION VIEWS OF THE SAME IMAGE
                for i, ID in enumerate(indexes[len(indexes) // 2:]):
                    X[i+len(indexes) // 2,], Y[len(indexes)-i-1,] = self.process_image(self.data_list.iloc[ID])
                    z[i+len(indexes) // 2,] = 1 # HALF OF THE BATCH IS PREPARED WITH THE DIFFERENT AUGMENTATION VIEWS OF THE DIFFERENT IMAGES
                return [X, Y], z

    def generator(self):
        '''THIS FUNCTION GENERATES ONE BATCH OF DATA FOR MULTIPLE GPU LEARNING'''
        while True:
            start=0
            end=self.batch_size
            X = np.empty((self.batch_size, *self.image_shape))
            Y = np.empty((self.batch_size, *self.mask_shape))
            z = np.empty((self.batch_size, 1))
            while start < self.__len__():
                indexes = self.indexes[start : end]
                if not self.self_supervised: # IF STANDARD SUPERVISED LEARNING, RETURNS ONLY IMAGE AND MASK PAIRS
                    for i, ID in enumerate(indexes):
                        X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
                    yield X, Y
                else: # IF SELF SUPERVISED LEARNING
                    if self.self_reconstruct:  # IF SELF-RECONSTRUCTION LEARNING, RETURNS ONLY IMAGE - IMAGE PAIRS
                        for i, ID in enumerate(indexes):
                            X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
                        yield X, Y
                    else: # IF SIMCLR LEARNING, RETURNS IMAGE - IMAGE PAIRS TOGETHER WITH BINARY LABELS
                        for i, ID in enumerate(indexes[:len(indexes) // 2]):
                            X[i,], Y[i,] = self.process_image(self.data_list.iloc[ID])
                            z[i,] = 0 # HALF OF THE BATCH IS PREPARED WITH THE DIFFERENT AUGMENTATION VIEWS OF THE SAME IMAGE
                        for i, ID in enumerate(indexes[len(indexes) // 2:]):
                            X[i + len(indexes) // 2,], Y[len(indexes) - i - 1,] = self.process_image(self.data_list.iloc[ID])
                            z[i + len(indexes) // 2,] = 1 # HALF OF THE BATCH IS PREPARED WITH THE DIFFERENT AUGMENTATION VIEWS OF THE DIFFERENT IMAGES
                        yield [X, Y], z

                start += self.batch_size
                end += self.batch_size


    def process_image(self, data_ID):
        '''
        THIS FUNCTION LOAD AN IMAGE AND MAKE TRANSFORMATION ON IT.
        :param data_ID: index id of the image to be loaded for the batch
        :return: returns transformed image and its mask
        '''
        image, mask = self.load_data(data_ID)

        if self.transform is not None:
            if self.self_supervised:
                image = self.transform(image=image)['image']
                mask = self.transform(image=mask)['image']
            else:
                augmented = self.transform(image=image, mask=mask)
                image=augmented['image']
                mask=augmented['mask']

        return image , mask

    def load_data(self, data):
        '''
        THIS FUNCTION LOAD AN IMAGE AND MAKE TRANSFORMATION ON IT.
        :param data: index id of the image to be loaded for the batch
        :return: returns image and its mask
        '''
        image = img_to_array(load_img(data['image'], color_mode=self.image_color_mode)).astype(np.uint8)
        if self.self_supervised:
            return image,image
        else:
            mask = (img_to_array(load_img(data['mask'], color_mode=self.mask_color_mode)) / 255).astype(np.uint8)
            return image, mask



#######################################################################################################################


class InstanceGenerator(Sequence):
    """
    THIS CLASS BUILDS A DATA GENERATOR FOR UNET-BASED ARCHITECTURES.
    IT CAN BE USED FOR ONLY EVALUATION PURPOSES FOR SINGLE IMAGE OR A GROUP OF IMAGES IN A FOLDER.
    """
    def __init__(self, transform, image_path, label_path=None, format='bmp'):
        self.transform = transform
        self.is_labelled = False if label_path is None else True

        if Path(image_path).is_dir():
            self.files = glob.glob(image_path + '/*.{}'.format(format))
            if self.is_labelled: self.labels = glob.glob(label_path + '/*.{}'.format(format))
        else:
            self.files = [image_path]
            if self.is_labelled: self.labels = [label_path]

    def __len__(self):
        '''THIS FUNCTION DENOTES THE NUMBER OF BATCHES PER EPOCH.'''
        return len(self.files)

    def __getitem__(self, index):
        '''THIS FUNCTION GENERATES ONE BATCH OF DATA FOR SINGLE GPU LEARNING'''
        image = img_to_array(load_img(self.files[index])).astype(np.uint8)
        transformed=np.expand_dims(self.transform(image=image)['image'], axis=0)
        if self.is_labelled:
            return transformed, self.files[index], self.labels[index]
        else:
            return transformed, self.files[index], None
