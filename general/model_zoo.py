"""
This code is generated by Ridvan Salih KUZU @DLR
LAST EDITED:  01.06.2021
ABOUT SCRIPT:
It contains some custom metric functions exploited for experimental purposes in this project.
"""

import tensorflow as tf
from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow_examples.models.pix2pix import pix2pix
from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D
from tensorflow.keras import backend as K
from keras_unet_collection import models

def get_sim_clr_model(base_model, input_shape):
    '''
      THIS FUNCTION RETURNS SIM-CLR MODEL.
      :param base_model: backbone UNET model
      :param input_shape: (width,height,channel) shape of the input images
      :return: returns simCLR model
     '''
    return SimCLR(base_model,input_shape)

def get_model(model_type, input_shape, is_freeze=False):
    '''
    THIS FUNCTION RETURNS a UNET MODEL VARIANT.
    :param model_type: model architecture index; 0: MobileUNet, 1: UNet, 2: ResUNet, 3:AttentionUNet with ResNet, 4:AttentionUNet with EfficientNet-B0, 5:AttentionUNet with EfficientNet-B7, 6:Attention UNet without backbone
    :param input_shape: (width,height,channel) shape of the input images
    :param is_freeze: it determine if the backbone will be frozen or not in training
    :return: returns a UNET model variant
    '''


    if model_type == 0:
        return MobileUnet(input_shape=input_shape, freeze_backbone= is_freeze)
    elif model_type == 1:
        return models.u2net_2d(input_shape, n_labels=1, filter_num_down=[64, 128, 256], activation='ReLU',
                               output_activation='Sigmoid',
                               batch_norm=True, pool=False, unpool=False, deep_supervision=True, name='u2net')
    elif model_type == 2:
        return models.resunet_a_2d(input_shape, [32, 64, 128, 256, 512], dilation_num=[1, 3, 5, 7], n_labels=1,
                                   aspp_num_down=256, aspp_num_up=128,
                                   activation='ReLU', output_activation='Sigmoid', batch_norm=True, pool=False,
                                   unpool='bilinear', name='resunet')
    elif model_type == 3:
        return models.att_unet_2d(input_shape, [32, 64, 128, 256, 512], n_labels=1, stack_num_down=5, stack_num_up=5,
                                  backbone='ResNet50', activation='ReLU',
                                  atten_activation='ReLU', attention='multiply', output_activation='Sigmoid', batch_norm=True,
                                  pool=False, freeze_batch_norm=False, freeze_backbone=is_freeze,
                                  unpool='bilinear', name='attunet')
    elif model_type == 4:
        return models.att_unet_2d(input_shape, [32, 64, 128, 256, 512], n_labels=1, stack_num_down=5, stack_num_up=5,
                                  backbone='EfficientNetB0', activation='ReLU', atten_activation='ReLU',
                                  attention='multiply', output_activation='Sigmoid', batch_norm=True,
                                  pool=False, freeze_backbone=is_freeze, freeze_batch_norm=False, unpool='bilinear',
                                  name='attunet')

    elif model_type == 5:
        return models.att_unet_2d(input_shape, [32, 64, 128, 256, 512], n_labels=1, stack_num_down=5, stack_num_up=5,
                                  backbone='EfficientNetB7', activation='ReLU', atten_activation='ReLU',
                                  attention='multiply', output_activation='Sigmoid', batch_norm=True,
                                  pool=False, freeze_backbone=is_freeze, freeze_batch_norm=False, unpool='bilinear',
                                  name='attunet')

    elif model_type == 6:
        return models.att_unet_2d(input_shape, [32, 64, 128, 256, 512], n_labels=1, stack_num_down=5, stack_num_up=5,
                                  activation='ReLU', atten_activation='ReLU',
                                  attention='multiply', output_activation='Sigmoid', batch_norm=True,
                                  pool=False, freeze_batch_norm=False, unpool='bilinear',
                                  name='attunet')

#####################################################################################################################

class SimCLR(tf.keras.Model):

    def __init__(self, base_model, input_shape, order=0):
        '''
        THIS IS THE CONSTRUCTOR OF SIM-CLR CLASS.
        :param base_model: backbone UNET model; it may not workwith some of the base models, model type 0, 5, and 6 are valid choices
        :param input_shape: (width,height,channel) shape of the input images
        :param order: it determines where to cut (from which layer order) UNET model for putting CNN + MLP header
        :return: returns nothing
        '''
        input_a = Input(shape=input_shape)
        input_b = Input(shape=input_shape)

        # CUT UNET MODEL FROM A CERTAIN POINT
        if base_model.name=='attunet_model':
            layer_name='attunet_up{}_conv_after_concat_4_activation'.format(order) #'attunet_up0_conv_after_concat_0_bn'
            neurons = base_model.get_layer(layer_name).output_shape[1:][2]
        elif base_model.name=='mobile_unet':
            order=order+2
            layer_name = 'concatenate_{}'.format(order)
            neurons = base_model.get_layer(layer_name).output_shape[1:][2]


        for layer in base_model.layers: layer.trainable = False

        for ind, layer in enumerate(base_model.layers):
            base_model.layers[ind].trainable=True
            if layer.name == layer_name:
                break

        # CNN + MLP HEADER DEFINITION
        model_h = Sequential([
            Input(shape=base_model.get_layer(layer_name).output_shape[1:]),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(neurons, 3, activation='swish', padding='same', kernel_initializer='he_normal'),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(neurons//2, 3, activation='swish', padding='same', kernel_initializer='he_normal'),
            MaxPooling2D(pool_size=(2, 2)),
            Flatten(),
            Dense(2048, activation='swish'),
            Dropout(0.5),
            Dense(512, activation='swish'),
            Lambda(lambda x: K.l2_normalize(x, axis=-1))

        ])

        composed_model = Model(inputs=[base_model.input], outputs=[model_h(base_model.get_layer(layer_name).output)])

        out_a = composed_model(input_a)
        out_b = composed_model(input_b)

        # CONCATENATED OUTPUT FOR SIM-CLR MODEL
        con_out = tf.keras.layers.Concatenate(axis=-1,name='contrastive')([out_a,out_b])

        # BINARY OUTPUT FOR SELF-SUPERVISED MODEL
        out = tf.math.subtract(out_a, out_b, name=None)
        out = Lambda(lambda out: tf.norm(out, ord='euclidean', keepdims=True, axis=-1))(out)
        binary =Dense(1, activation='sigmoid', name='binary')(out)


        super(SimCLR, self).__init__(inputs=[input_a, input_b], outputs=[con_out,binary])
        self.base_model = base_model


#############################################################################################################

class MobileUnet(tf.keras.Model):

    def __init__(self, input_shape, freeze_backbone=False, weights=None):
        '''
        THIS IS THE CONSTRUCTOR OF MOBILE UNET CLASS.
        :param input_shape: (width,height,channel) shape of the input images
        :param freeze_backbone: it determines if the backbone model will be frozen or not
        :param weights: a file path for pretrained initialization of the model weights
        :return: returns nothing
               '''
        base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')

        # Use the activations of these layers
        layer_names = [
            'block_1_expand_relu',  # 64x64
            'block_3_expand_relu',  # 32x32
            'block_6_expand_relu',  # 16x16
            'block_13_expand_relu',  # 8x8
            'block_16_project',  # 4x4
        ]
        layers = [base_model.get_layer(name).output for name in layer_names]

        # Create the feature extraction model
        down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)
        down_stack.trainable = not freeze_backbone

        up_stack = [
            pix2pix.upsample(512, 3),  # 4x4 -> 8x8
            pix2pix.upsample(256, 3),  # 8x8 -> 16x16
            pix2pix.upsample(128, 3),  # 16x16 -> 32x32
            pix2pix.upsample(64, 3),  # 32x32 -> 64x64
        ]

        inputs = tf.keras.layers.Input(shape=input_shape)
        x = inputs

        # Downsampling through the model
        skips = down_stack(x)
        x = skips[-1]
        skips = reversed(skips[:-1])

        # Upsampling and establishing the skip connections
        for up, skip in zip(up_stack, skips):
            x = up(x)
            concat = tf.keras.layers.Concatenate()
            x = concat([x, skip])

        # This is the last layer of the model
        last = tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding='same', activation='sigmoid')  # 64x64 -> 128x128 ,
        x = last(x)

        super(MobileUnet, self).__init__(inputs=inputs, outputs=x)

        if weights is not None:
            self.load_weights(filepath=weights)

            print('PRE-TIRAINED MODEL IS LOADED: {}'.format(weights))


########################################################################################################

class Unet(tf.keras.Model):
    def __init__(self, input_shape, freeze_backbone=False, weights=None):
        '''
        THIS IS THE CONSTRUCTOR OF UNET CLASS DEVELOPED BY Darshan Kishorbai Thummar.
        :param input_shape: (width,height,channel) shape of the input images
        :param freeze_backbone: it determines if the backbone model will be frozen or not, it is redundant in this class
        :param weights: a file path for pretrained initialization of the model weights
        :return: returns nothing
                      '''

        inputs = Input(input_shape)
        if input_shape[0]==512: nu_layer=8
        elif input_shape[0]==256: nu_layer=4
        else: raise NotImplementedError

        neurons = int(input_shape[0] /nu_layer)

        conv1 = Conv2D(neurons, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)
        # conv1 = BatchNormalization()(conv1)
        conv1 = Conv2D(neurons, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)
        # conv1 = BatchNormalization()(conv1)
        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

        conv2 = Conv2D(neurons * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)
        # conv2 = BatchNormalization()(conv2)
        conv2 = Conv2D(neurons * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)
        # conv2 = BatchNormalization()(conv2)
        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

        conv3 = Conv2D(neurons * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)
        # conv3 = BatchNormalization()(conv3)
        conv3 = Conv2D(neurons * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)
        # conv3 = BatchNormalization()(conv3)
        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

        conv4 = Conv2D(neurons * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)
        # conv4 = BatchNormalization()(conv4)
        conv4 = Conv2D(neurons * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)
        # conv4 = BatchNormalization()(conv4)
        drop4 = Dropout(0.5)(conv4)
        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

        conv5 = Conv2D(neurons * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)
        # conv5 = BatchNormalization()(conv5)
        conv5 = Conv2D(neurons * 16, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)
        # conv5 = BatchNormalization()(conv5)
        drop5 = Dropout(0.5)(conv5)

        up6 = Conv2D(neurons * 8, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
            UpSampling2D(size=(2, 2))(drop5))
        # up6 = BatchNormalization()(up6)
        merge6 = concatenate([drop4, up6], axis=3)
        conv6 = Conv2D(neurons * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)
        # conv6 = BatchNormalization()(conv6)
        conv6 = Conv2D(neurons * 8, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)
        # conv6 = BatchNormalization()(conv6)

        up7 = Conv2D(neurons * 4, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
            UpSampling2D(size=(2, 2))(conv6))
        # up7 = BatchNormalization()(up7)
        merge7 = concatenate([conv3, up7], axis=3)
        conv7 = Conv2D(neurons * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)
        # conv7 = BatchNormalization()(conv7)
        conv7 = Conv2D(neurons * 4, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)
        # conv7 = BatchNormalization()(conv7)

        up8 = Conv2D(neurons * 2, 2, activation='relu', padding='same', kernel_initializer='he_normal')(
            UpSampling2D(size=(2, 2))(conv7))
        # up8 = BatchNormalization()(up8)

        merge8 = concatenate([conv2, up8], axis=3)
        conv8 = Conv2D(neurons * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)
        # conv8 = BatchNormalization()(conv8)
        conv8 = Conv2D(neurons * 2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)
        # conv8 = BatchNormalization()(conv8)


        last = tf.keras.layers.Conv2DTranspose(1, 3, strides=2, padding='same', activation='sigmoid')
        x = last(conv8)

        super(Unet, self).__init__(inputs=inputs, outputs=x)

        if weights is not None:
            self.load_weights(filepath=weights)

            print('PRE-TIRAINED MODEL IS LOADED: {}'.format(weights))
