"""
This code is generated by Ridvan Salih KUZU @DLR
LAST EDITED:  01.06.2021
ABOUT SCRIPT:
It contains some custom loss functions exploited for experimental purposes in this project.
"""
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Lambda, Dense

def mean_iou_loss():
    '''
    THIS FUNCTION CALCULATES MEAN INTERSECTION OVER UNION METRICS BETWEEN PREDICTED AND GROUND-TRUTH LABELS.
    '''
    def iou(y_true,y_pred):
        '''
        THIS FUNCTION CALCULATES MEAN INTERSECTION OVER UNION METRICS BETWEEN PREDICTED AND GROUND-TRUTH LABELS.
        :param y_true: ground-truth labels
        :param y_pred: predicted labels
        :return: returns the metrics value
        '''
        y_pred = tf.cast(tf.cast(y_pred + 0.5, tf.int32),tf.float32)
        intersection = K.sum(y_true * y_pred)
        # calculate the |union| (OR) of the labels
        union = K.sum(y_true) + K.sum(y_pred) - intersection
        # avoid divide by zero - if the union is zero, return 1
        # otherwise, return the intersection over union
        return 1-K.switch(K.equal(union, 0), 1.0, intersection / union)


    return iou


def dice(is_hard=True):
    def dice_hard(y_true, y_pred, threshold=0.5, axis=[1, 2, 3], smooth=1e-5):
        """Non-differentiable Sørensen–Dice coefficient for comparing the similarity
        of two batch of data, usually be used for binary image segmentation i.e. labels are binary.
        The coefficient between 0 to 1, 1 if totally match.
        Parameters
        -----------
        y_pred : tensor
            A distribution with shape: [batch_size, ....], (any dimensions).
        y_true : tensor
            A distribution with shape: [batch_size, ....], (any dimensions).
        threshold : float
            The threshold value to be true.
        axis : list of integer
            All dimensions are reduced, default ``[1,2,3]``.
        smooth : float
            This small value will be added to the numerator and denominator, see ``dice_coe``.
        References
        -----------
        - `Wiki-Dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`_
        """
        y_pred = tf.cast(y_pred > threshold, dtype=tf.float32)
        y_true = tf.cast(y_true > threshold, dtype=tf.float32)
        inse = tf.reduce_sum(tf.multiply(y_pred, y_true), axis=axis)
        l = tf.reduce_sum(y_pred, axis=axis)
        r = tf.reduce_sum(y_true, axis=axis)
        ## old axis=[0,1,2,3]
        # hard_dice = 2 * (inse) / (l + r)
        # epsilon = 1e-5
        # hard_dice = tf.clip_by_value(hard_dice, 0, 1.0-epsilon)
        ## new haodong
        hard_dice = (2. * inse + smooth) / (l + r + smooth)
        ##
        hard_dice = tf.reduce_mean(hard_dice)
        return hard_dice

    def dice_soft(y_true, y_pred, loss_type='sorensen', axis=[1,2,3], smooth=1e-5, from_logits=True):
        """Soft dice (Sørensen or Jaccard) coefficient for comparing the similarity
        of two batch of data, usually be used for binary image segmentation
        i.e. labels are binary. The coefficient between 0 to 1, 1 means totally match.
        Parameters
        -----------
        y_pred : tensor
            A distribution with shape: [batch_size, ....], (any dimensions).
        y_true : tensor
            A distribution with shape: [batch_size, ....], (any dimensions).
        loss_type : string
            ``jaccard`` or ``sorensen``, default is ``jaccard``.
        axis : list of integer
            All dimensions are reduced, default ``[1,2,3]``.
        smooth : float
            This small value will be added to the numerator and denominator.
            If both y_pred and y_true are empty, it makes sure dice is 1.
            If either y_pred or y_true are empty (all pixels are background), dice = ```smooth/(small_value + smooth)``,
            then if smooth is very small, dice close to 0 (even the image values lower than the threshold),
            so in this case, higher smooth can have a higher dice.
        Examples
        ---------
        >>> outputs = tl.act.pixel_wise_softmax(network.outputs)
        >>> dice_loss = 1 - tl.cost.dice_coe(outputs, y_)
        References
        -----------
        - `Wiki-Dice <https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient>`_
        """

        if not from_logits:
            # transform back to logits
            _epsilon = tf.convert_to_tensor(1e-7, y_pred.dtype.base_dtype)
            y_pred = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)
            y_pred = tf.math.log(y_pred / (1 - y_pred))

        inse = tf.reduce_sum(y_pred * y_true, axis=axis)
        if loss_type == 'jaccard':
            l = tf.reduce_sum(y_pred * y_pred, axis=axis)
            r = tf.reduce_sum(y_true * y_true, axis=axis)
        elif loss_type == 'sorensen':
            l = tf.reduce_sum(y_pred, axis=axis)
            r = tf.reduce_sum(y_true, axis=axis)
        else:
            raise Exception("Unknow loss_type")
        ## old axis=[0,1,2,3]
        # dice = 2 * (inse) / (l + r)
        # epsilon = 1e-5
        # dice = tf.clip_by_value(dice, 0, 1.0-epsilon) # if all empty, dice = 1
        ## new haodong
        dice = (2. * inse + smooth) / (l + r + smooth)
        ##
        dice = tf.reduce_mean(dice)
        return dice
    if is_hard:
        return dice_hard
    return dice_soft


def dice_loss(y_true, y_pred):
    '''
    THIS FUNCTION IS EXPERIMENTAL.
    THIS FUNCTION CALCULATES THE DICE LOSS BETWEEN PREDICTED AND GROUND-TRUTH LABELS.
    :param y_true: ground-truth labels
    :param y_pred: predicted labels
    :return: returns the loss value
    '''
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.math.sigmoid(y_pred)
    numerator = 2 * tf.reduce_sum(y_true * y_pred)
    denominator = tf.reduce_sum(y_true + y_pred)

    return 1 - numerator / denominator

def weighted_binary_crossentropy_loss(pos_weight):
    # pos_weight: A coefficient to use on the positive examples.
    def weighted_binary_crossentropy(target, output, from_logits=True):
        """Binary crossentropy between an output tensor and a target tensor.
        # Arguments
            target: A tensor with the same shape as `output`.
            output: A tensor.
            from_logits: Whether `output` is expected to be a logits tensor.
                By default, we consider that `output`
                encodes a probability distribution.
        # Returns
            A tensor.
        """
        # Note: tf.nn.sigmoid_cross_entropy_with_logits
        # expects logits, Keras expects probabilities.
        if not from_logits:
            # transform back to logits
            _epsilon = tf.convert_to_tensor(1e-7, output.dtype.base_dtype)
            output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)
            output = tf.math.log(output / (1 - output))

        return tf.nn.weighted_cross_entropy_with_logits(labels=target,
                                                       logits=output,
                                                        pos_weight=pos_weight)
    return weighted_binary_crossentropy


def cross_entropy_dice_loss(y_true, y_pred):
    '''
    THIS FUNCTION IS EXPERIMENTAL.
    THIS FUNCTION CALCULATES THE CROSS-ENTROPY DICE LOSS BETWEEN PREDICTED AND GROUND-TRUTH LABELS.
    :param y_true: ground-truth labels
    :param y_pred: predicted labels
    :return: returns the loss value
    '''
    y_true = tf.cast(y_true, tf.float32)
    o = tf.keras.losses.categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)
    return tf.reduce_mean(o)

def contrastive_loss(y, preds, margin=1):
    '''
    THIS FUNCTION IS EXPERIMENTAL.
    THIS FUNCTION CALCULATES THE CONTRASTIVE LOSS BETWEEN PREDICTED AND GROUND-TRUTH LABELS.
    :param y_true: ground-truth labels
    :param y_pred: predicted labels
    :return: returns the loss value
    '''
    # explicitly cast the true class label data type to the predicted
    # class label data type (otherwise we run the risk of having two
    # separate data types, causing TensorFlow to error out)
    y = tf.cast(y, preds.dtype)
    # calculate the contrastive loss between the true labels and
    # the predicted labels
    squaredPreds = K.square(preds)
    squaredMargin = K.square(K.maximum(margin - preds, 0))
    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)
    # return the computed contrastive loss to the calling function
    return loss


def self_supervised_binary_cross_entropy(name='cross_entropy'):
    '''
    THIS FUNCTION IS EXPERIMENTAL.
    THIS FUNCTION CALCULATES BINARY CROSS ENTROPY FOR SELF-SUPERVISED LEARNING.
    '''
    dense=Dense(1, activation='sigmoid', name='binary')
    binary_cross=tf.keras.losses.BinaryCrossentropy()
    def func(y_true,y_pred):
        '''
        THIS FUNCTION CALCULATES BINARY CROSS ENTROPY.
        :param y_true: ground-truth labels
        :param y_pred: predicted labels
        :return: returns the loss value
        '''
        out_a, out_b = tf.split(y_pred, 2, -1)
        out = tf.math.subtract(out_a, out_b, name=None)
        out = Lambda(lambda out: tf.norm(out, ord='euclidean', keepdims=True, axis=-1))(out)
        #out= -K.mean(out_a * out_b, axis=-1, keepdims=True)
        prediction = dense(out)
        return binary_cross(y_true, prediction)

    func.__name__ = '{}'.format(name)
    return func

#################################################################################################

class NTXent(tf.keras.losses.Loss):
    '''
    THIS CLASS IMPLEMENTS Normalized Temperature-scaled Cross Entropy Loss FOR SIM-CLR BASED SELF-SUPERVISED LEARNING.
    THE DETAILS CAN BE FOUND HERE: https://paperswithcode.com/method/nt-xent
    '''

    def __init__(self, tau=0.1):
        '''
        THIS IS THE CONSTRUCTOR OF THE CLASS.
        :param tau: temperature parameter
        :return: returns nothing
        '''
        super().__init__()
        self.tau = tau
        self.criterion = tf.keras.losses.CategoricalCrossentropy(from_logits=True)


    @tf.function
    def calculate_loss(self, hidden):
        '''
        THIS FUNCTION CALCULATES Temperature-scaled Cross Entropy Loss.
        :param hidden: concatenated output of the SimCLR architecture
        :return: returns the loss value
        '''
        LARGE_NUM = 1e9
        # hidden = tf.math.l2_normalize(hidden, -1)
        hidden1, hidden2 = tf.split(hidden, 2, -1)

        hidden1 = tf.math.l2_normalize(hidden1, -1)
        hidden2 = tf.math.l2_normalize(hidden2, -1)

        batch_size = tf.shape(hidden1)[0]

        # Gather hidden1/hidden2 across replicas and create local labels.
        hidden1_large = hidden1
        hidden2_large = hidden2
        labels = tf.one_hot(tf.range(batch_size), batch_size * 2)
        masks = tf.one_hot(tf.range(batch_size), batch_size)

        logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=True) / self.tau
        logits_aa = logits_aa - masks * LARGE_NUM
        logits_bb = tf.matmul(hidden2, hidden2_large, transpose_b=True) / self.tau
        logits_bb = logits_bb - masks * LARGE_NUM
        logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=True) / self.tau
        logits_ba = tf.matmul(hidden2, hidden1_large, transpose_b=True) / self.tau

        loss_a = self.criterion(labels, tf.concat([logits_ab, logits_aa], 1))
        loss_b = self.criterion(labels, tf.concat([logits_ba, logits_bb], 1))
        loss = loss_a + loss_b
        return loss

    def call(self, y_true, y_pred):
        '''
        THIS FUNCTION CALCULATES Temperature-scaled Cross Entropy Loss.
        :param y_true: ground-truth labels, it is redundant in this loss calculation but preserved for possible future use cases
        :param y_pred: predicted labels
        :return: returns the loss value
        '''
        y_pred_1, y_pred_2 = tf.split(y_pred, 2, 0)
        return self.calculate_loss(y_pred_1)
